{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU-y74K14q6S"
      },
      "source": [
        "####Installing anvil-uplink to allow connection to our Anvil project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Hi_CpkmlQXyD",
        "outputId": "c3475944-345d-4f15-916a-fa84661f2f7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anvil-uplink\n",
            "  Downloading anvil_uplink-0.5.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting argparse (from anvil-uplink)\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from anvil-uplink) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from anvil-uplink) (1.17.0)\n",
            "Collecting ws4py-sslupdate (from anvil-uplink)\n",
            "  Downloading ws4py_sslupdate-0.5.1b0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading anvil_uplink-0.5.2-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Downloading ws4py_sslupdate-0.5.1b0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ws4py-sslupdate, argparse, anvil-uplink\n",
            "Successfully installed anvil-uplink-0.5.2 argparse-1.4.0 ws4py-sslupdate-0.5.1b0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse",
                  "google"
                ]
              },
              "id": "332269984a484850989dde2353e20ae0"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install anvil-uplink"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MhN3nKBesff"
      },
      "source": [
        "###Importing the necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFa2fLuRlDQO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import random\n",
        "import io\n",
        "import base64\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qxm-vRE4ew4D"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sD-F_zJ8AoZt",
        "outputId": "543b67d1-efaa-4b35-e075-75ed437f4090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to wss://anvil.works/uplink\n",
            "Anvil websocket open\n",
            "Connected to \"Default Environment\" as SERVER\n"
          ]
        }
      ],
      "source": [
        "import anvil.server\n",
        "#Key must be hidden, only visible before deployment\n",
        "anvil.server.connect(\"server_DTCDOGSFUVQE5ZWTRAZ2BK4C-4CTB26IRHB44WTGL\")\n",
        "\n",
        "\n",
        "\n",
        "df = None\n",
        "target = None\n",
        "problem_type = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBul0aZD4z4K"
      },
      "source": [
        "###Reading the csv file, storing it in a temp file and returning its details to anvil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjBvRUH2A2Cn"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def process_csv(media_file):\n",
        "\n",
        "    global df\n",
        "    # Save the uploaded file to a temporary directory in Colab\n",
        "    file_name = media_file.name\n",
        "    file_path = f\"/tmp/{file_name}\"\n",
        "\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(media_file.get_bytes())\n",
        "\n",
        "    # Read the CSV file using pandas\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Extract file details\n",
        "    num_rows = df.shape[0]  # Number of rows\n",
        "    num_columns = df.shape[1]  # Number of columns\n",
        "    column_names = list(df.columns)  # Column names\n",
        "\n",
        "    # Print file details in the Colab output for debugging\n",
        "    print(f\"File '{file_name}' details:\")\n",
        "    print(f\"Number of rows: {num_rows}\")\n",
        "    print(f\"Number of columns: {num_columns}\")\n",
        "    print(f\"Column names: {column_names}\")\n",
        "\n",
        "    # Return the file details to the Anvil client\n",
        "    return {\n",
        "        \"file_name\": file_name,\n",
        "        \"num_rows\": num_rows,\n",
        "        \"num_columns\": num_columns,\n",
        "        \"column_names\": column_names\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtrVIrOQ4_Wz"
      },
      "source": [
        "###Identifying the problem (classification/regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG1mkdCqf_gq"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def process_selected_column(selected_column):\n",
        "    global df\n",
        "    global target\n",
        "    target = df[selected_column] if df is not None else None  # Ensure target is not None\n",
        "    global problem_type\n",
        "\n",
        "    if df is None:\n",
        "        return \"No data loaded\"\n",
        "\n",
        "    if selected_column not in df.columns:\n",
        "        return f\"Column '{selected_column}' not found in the data\"\n",
        "\n",
        "    # Ensure target is not None\n",
        "    if target is None:\n",
        "        return f\"Error: Column '{selected_column}' is empty or invalid.\"\n",
        "\n",
        "    # Properties to determine if it's regression/classification\n",
        "    column_type = target.dtype\n",
        "    unique_values = target.nunique()\n",
        "\n",
        "    # If the column is type string or boolean it is classification\n",
        "    if pd.api.types.is_string_dtype(target) or pd.api.types.is_bool_dtype(target) or is_boolean_numerical(target):\n",
        "        problem_type = \"classification\"\n",
        "\n",
        "    # If the column is numerical\n",
        "    elif pd.api.types.is_numeric_dtype(target) or pd.api.types.is_float_dtype(target):\n",
        "      problem_type = \"regression\"\n",
        "\n",
        "    else:\n",
        "        problem_type = \"Could not determine\"\n",
        "\n",
        "    print(column_type)\n",
        "    print(f\"Selected Column: {selected_column}\")\n",
        "    print(f\"Problem Type: {problem_type}\")\n",
        "\n",
        "    # Return the results for Anvil\n",
        "    return {\n",
        "        'problem_type': problem_type,\n",
        "    }\n",
        "\n",
        "def is_boolean_numerical(column):\n",
        "    unique_values = column.dropna().unique()\n",
        "    return set(unique_values).issubset({0, 1})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKllqtcF-Jxt"
      },
      "source": [
        "###This cell retuns the number of null values in all columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67xkllnds-vv"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def null_count():\n",
        "  global df\n",
        "  if df is None:\n",
        "    return \"No data loaded\"\n",
        "\n",
        "  #if selected_column not in df.columns:\n",
        "   # return f\"Column '{selected_column}' not found in the data\"\n",
        "\n",
        "  null_count = df.isnull().sum().to_dict()\n",
        "\n",
        "\n",
        "  print(f\"Number of nulls in is: {null_count}\")\n",
        "\n",
        "  return {\n",
        "        \"null_count\": null_count\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzA557Hs-Vee"
      },
      "source": [
        "###This cell returns the number of blank values in all columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hE6i0Ok3m7IQ"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def blank_count():\n",
        "    global df\n",
        "    if df is None:\n",
        "        return {\"error\": \"No data loaded\"}\n",
        "\n",
        "    # Count blanks for each column\n",
        "    blank_count = (df == '').sum()  # This will return a Series\n",
        "    blank_count_dict = blank_count.to_dict()  # Convert Series to dictionary\n",
        "\n",
        "    print(f\"Number of blanks in each column: {blank_count_dict}\")\n",
        "\n",
        "    return {\n",
        "        \"blank_count\": blank_count_dict\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpPiaOIBBj12"
      },
      "source": [
        "###This cell fills all null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPSGolkkBmk0"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def fill_nulls():\n",
        "    global df\n",
        "\n",
        "    if df is None:\n",
        "        return \"No data loaded\"\n",
        "\n",
        "    # Iterate through each column in the DataFrame\n",
        "    for column in df.columns:\n",
        "        # If the column is numeric, fill nulls with the mean\n",
        "        if pd.api.types.is_numeric_dtype(df[column]):\n",
        "            avg_value = df[column].mean()\n",
        "            df[column].fillna(avg_value, inplace=True)\n",
        "\n",
        "        # If the column is string or boolean, fill nulls with the mode\n",
        "        elif pd.api.types.is_string_dtype(df[column]) or pd.api.types.is_bool_dtype(df[column]):\n",
        "            mode_value = df[column].mode()[0]\n",
        "            df[column].fillna(mode_value, inplace=True)\n",
        "\n",
        "    # Return to ensure the method executed\n",
        "    return \"success\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTPUG4xCPruV"
      },
      "source": [
        "###This cell removes Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKe5cXMCPubV"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def remove_duplicates():\n",
        "  global df\n",
        "\n",
        "  if df is None:\n",
        "    return \"No data loaded\"\n",
        "\n",
        "  df.drop_duplicates(inplace=True)\n",
        "\n",
        "  return \"Removed Duplicates\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORle-2qS_ZPO"
      },
      "source": [
        "*This* cell returns the number of outliers in certain columns\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEoxccsbsGpS"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def detect_outliers():\n",
        "    global df\n",
        "    if df is None:\n",
        "        return \"No data loaded\"\n",
        "\n",
        "    for column in df.select_dtypes(include=[np.number]).columns:\n",
        "        Q1 = df[column].quantile(0.25)\n",
        "        Q3 = df[column].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
        "        outlier_count = outliers.shape[0]\n",
        "        outlier_results= {\n",
        "            \"The total number of outliers is\": outlier_count,\n",
        "            #\"outliers\": outliers[column].tolist()  # List of outlier values\n",
        "        }\n",
        "\n",
        "    return { \"outlier_results\" :outlier_results}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0sBW4YeZuFT"
      },
      "source": [
        "This cell counts the duplicates in all columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7o7CGLYZrvy"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def detect_duplicates():\n",
        "    global df  # Ensure you're using the global DataFrame\n",
        "\n",
        "    if df is None:\n",
        "        return \"No data loaded\"\n",
        "\n",
        "    # Count the number of duplicates in the selected column\n",
        "    duplicates = df.duplicated().sum()\n",
        "\n",
        "    #total_duplicate_count = df[selected_column].value_counts()[df[selected_column].value_counts() > 1].sum()\n",
        "\n",
        "    #print(f\"Number of duplicate rows in column '{selected_column}': {duplicate_count}\")\n",
        "    #print(f\"Total count of duplicates in column '{selected_column}': {total_duplicate_count}\")\n",
        "\n",
        "    return {\"duplicates\": duplicates}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLqd7DTbp4Qf"
      },
      "source": [
        "###Checking for class imbalance if its a classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo4fUJr9qVtz"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def balanced_classes(selected_column):\n",
        "  global df\n",
        "  global problem_type\n",
        "  imbalance_threshold = 0.1\n",
        "\n",
        "  if selected_column not in df.columns:\n",
        "    return f\"Column '{selected_column}' not found in the data\"\n",
        "\n",
        "  target = df[selected_column]\n",
        "\n",
        "  #calculating the relative frequency out of 1.0\n",
        "  class_distribution = target.value_counts(normalize=True)\n",
        "\n",
        "  #Finding if theres class imbalance\n",
        "  imbalanced_classes = class_distribution[class_distribution < imbalance_threshold]\n",
        "\n",
        "  if imbalanced_classes.empty:\n",
        "    return True #Data is balanced\n",
        "  else:\n",
        "    return False #Data is not balanced\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkZnVwDcgi-1"
      },
      "outputs": [],
      "source": [
        "\n",
        "@anvil.server.callable\n",
        "def get_dataset():\n",
        "  global df\n",
        "  if df is None:\n",
        "    return \"No data loaded\"\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGDu2YsgygM1"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyBAJXl79VpC"
      },
      "outputs": [],
      "source": [
        "\n",
        "#pip install carbontracker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr1m5xpV4K5Q",
        "outputId": "f31c6035-62fd-4752-9cfe-2e002119799b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting codecarbon\n",
            "  Downloading codecarbon-2.8.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting arrow (from codecarbon)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from codecarbon) (8.1.8)\n",
            "Collecting fief-client[cli] (from codecarbon)\n",
            "  Downloading fief_client-0.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from codecarbon) (2.2.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from codecarbon) (0.21.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from codecarbon) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from codecarbon) (9.0.0)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from codecarbon) (11.4.1)\n",
            "Collecting questionary (from codecarbon)\n",
            "  Downloading questionary-2.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting rapidfuzz (from codecarbon)\n",
            "  Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from codecarbon) (2.32.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from codecarbon) (13.9.4)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (from codecarbon) (0.15.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from arrow->codecarbon) (2.8.2)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow->codecarbon)\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting httpx<0.28.0,>=0.21.3 (from fief-client[cli]->codecarbon)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jwcrypto<2.0.0,>=1.4 (from fief-client[cli]->codecarbon)\n",
            "  Downloading jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting yaspin (from fief-client[cli]->codecarbon)\n",
            "  Downloading yaspin-3.1.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->codecarbon) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->codecarbon) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->codecarbon) (2025.1)\n",
            "Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from questionary->codecarbon) (3.0.50)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->codecarbon) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->codecarbon) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer->codecarbon) (4.12.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer->codecarbon) (1.5.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\n",
            "Requirement already satisfied: cryptography>=3.4 in /usr/local/lib/python3.11/dist-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.17.0)\n",
            "Collecting termcolor<2.4.0,>=2.2.0 (from yaspin->fief-client[cli]->codecarbon)\n",
            "  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\n",
            "Downloading codecarbon-2.8.3-py3-none-any.whl (516 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.7/516.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading questionary-2.1.0-py3-none-any.whl (36 kB)\n",
            "Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jwcrypto-1.5.6-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Downloading fief_client-0.20.0-py3-none-any.whl (20 kB)\n",
            "Downloading yaspin-3.1.0-py3-none-any.whl (18 kB)\n",
            "Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
            "Installing collected packages: types-python-dateutil, termcolor, rapidfuzz, yaspin, questionary, httpx, arrow, jwcrypto, fief-client, codecarbon\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.5.0\n",
            "    Uninstalling termcolor-2.5.0:\n",
            "      Successfully uninstalled termcolor-2.5.0\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "Successfully installed arrow-1.3.0 codecarbon-2.8.3 fief-client-0.20.0 httpx-0.27.2 jwcrypto-1.5.6 questionary-2.1.0 rapidfuzz-3.12.2 termcolor-2.3.0 types-python-dateutil-2.9.0.20241206 yaspin-3.1.0\n"
          ]
        }
      ],
      "source": [
        "pip install codecarbon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yO44ULG0ow3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from codecarbon import EmissionsTracker\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "#from carbontracker.tracker import CarbonTracker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypRsP2GdPARZ"
      },
      "source": [
        "##Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0suuqoJOj2xH"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Use SGD-based SVM for large datasets\n",
        "def build_train_and_track_emissions_svm(X_train, y_train, X_val, y_val, output_size, input_size):\n",
        "    start_time = time.time()  # Start time tracking\n",
        "\n",
        "    # Use SGDClassifier for better efficiency on large data\n",
        "    model = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3, random_state=42, n_jobs=-1)\n",
        "\n",
        "    # Mini-batch training for large datasets\n",
        "    batch_size = 10000  # Adjust as needed\n",
        "    X_train, y_train = shuffle(X_train, y_train, random_state=42)  # Shuffle data\n",
        "\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        X_batch = X_train[i:i + batch_size]\n",
        "        y_batch = y_train[i:i + batch_size]\n",
        "        model.partial_fit(X_batch, y_batch, classes=np.unique(y_train))\n",
        "\n",
        "    # Stop time tracking\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    # Predictions on validation data\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    f_score = f1_score(y_val, y_pred, average='weighted')\n",
        "\n",
        "    # Carbon Emissions (if available)\n",
        "    try:\n",
        "        from codecarbon import EmissionsTracker\n",
        "        tracker = EmissionsTracker()\n",
        "        tracker.start()\n",
        "        emissions = tracker.stop() * 1000  # Convert to milligrams\n",
        "    except ImportError:\n",
        "        emissions = \"Tracking not enabled\"\n",
        "\n",
        "    # Results\n",
        "    result_output = (\n",
        "        f\"Estimated CO₂ emissions: {emissions} mg\\n\"\n",
        "        f\"Training Time: {total_time:.2f} seconds\\n\"\n",
        "        f\"Final Accuracy: {accuracy:.4f}\\n\"\n",
        "        f\"F-score: {f_score:.4f}\"\n",
        "    )\n",
        "\n",
        "    return result_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giSRgEeDhW2Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "\n",
        "# DECISION TREE\n",
        "\n",
        "def build_train_and_track_emissions_dt(X_train, y_train, X_val, y_val, input_size, output_size):\n",
        "    # Initialize the Decision Tree model with optimized parameters\n",
        "    model = DecisionTreeClassifier(\n",
        "        criterion=\"gini\",  # \"entropy\" can also be used\n",
        "        max_depth=None,  # Adjust for smaller datasets if needed\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Initialize CarbonTracker\n",
        "    tracker = EmissionsTracker()\n",
        "    tracker.start()\n",
        "\n",
        "    # Start tracking time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Stop tracking emissions\n",
        "    emissions = tracker.stop()\n",
        "    emissions = emissions * 1000  # Convert kg to grams\n",
        "\n",
        "    # Stop tracking time\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    # Get predictions\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Compute accuracy and F1-score\n",
        "    final_accuracy = accuracy_score(y_val, y_pred)\n",
        "    f_score = f1_score(y_val, y_pred, average=\"weighted\")\n",
        "\n",
        "    # Create a result string\n",
        "    result_output = (\n",
        "        f\"Estimated CO₂ emissions: {emissions:.6f} g\\n\"\n",
        "        f\"Time Taken: {total_time:.2f} seconds\\n\"\n",
        "        f\"Final Accuracy: {final_accuracy:.4f}\\n\"\n",
        "        f\"F-score: {f_score:.4f}\"\n",
        "    )\n",
        "\n",
        "    return result_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ie-NOuVBjTZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00195de9-0fa4-4319-e7d0-d02d60a027ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow.keras\n",
            "  Downloading tensorflow_keras-0.1-py3-none-any.whl.metadata (63 bytes)\n",
            "Downloading tensorflow_keras-0.1-py3-none-any.whl (5.2 kB)\n",
            "Installing collected packages: tensorflow.keras\n",
            "Successfully installed tensorflow.keras-0.1\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow.keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCcTiHVrjETJ"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "def build_train_and_track_emissions_nnn(X_train, y_train, X_val, y_val, input_size, output_size):\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(4, activation='relu', input_shape=(input_size,)),\n",
        "        layers.Dense(4, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['binary_accuracy'],\n",
        "    )\n",
        "\n",
        "    early_stopping = keras.callbacks.EarlyStopping(\n",
        "    patience=10,\n",
        "    min_delta=0.001,\n",
        "    restore_best_weights=True,\n",
        "    )\n",
        "\n",
        "    # Initialize CarbonTracker\n",
        "    tracker = EmissionsTracker()\n",
        "\n",
        "    # Start tracking carbon emissions\n",
        "    tracker.start()\n",
        "\n",
        "    # Start tracking time\n",
        "    start_time = time.time()\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        batch_size=512,\n",
        "        epochs=1000,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=0,  # Hide output\n",
        "    )\n",
        "\n",
        "    # Stop tracking emissions\n",
        "    emissions = tracker.stop() * 1000  # Convert to grams\n",
        "\n",
        "    # Stop tracking time\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    # Number of epochs used before early stopping\n",
        "    num_epochs = len(history.history['loss'])\n",
        "\n",
        "    # Get final accuracy\n",
        "    final_accuracy = history.history['binary_accuracy'][-1]\n",
        "\n",
        "    # Get F-score (using validation set predictions)\n",
        "    y_pred_prob = model.predict(X_val)\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to class labels\n",
        "    f_score = f1_score(y_val, y_pred, average='weighted')\n",
        "\n",
        "    # Create a result string\n",
        "    result_output = (\n",
        "        f\"Estimated CO₂ emissions: {emissions:.6f} g\\n\"\n",
        "        f\"Number of Epochs: {num_epochs}\\n\"\n",
        "        f\"Time Taken: {total_time:.2f} seconds\\n\"\n",
        "        f\"Final Accuracy: {final_accuracy:.4f}\\n\"\n",
        "        f\"F-score: {f_score:.4f}\"\n",
        "    )\n",
        "    return result_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BzZkVf8Gof1"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import sys\n",
        "import time\n",
        "from sklearn.metrics import f1_score\n",
        "# modified one (Acccuracy percentage)\n",
        "def build_train_and_track_emissions_nn(X_train, y_train, X_val, y_val, input_size, output_size):\n",
        "    hidden_layer_size = 50  # Number of neurons in the hidden layer\n",
        "\n",
        "    # Define the model.  modified one\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(hidden_layer_size, activation='relu', input_shape=(input_size,)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "        tf.keras.layers.Dense(output_size, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Early stopping to prevent overfitting\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(patience=25, restore_best_weights=True)\n",
        "\n",
        "    # Dynamic batch size: Smaller for small datasets, larger for big datasets\n",
        "    batch_size = min(64, max(16, len(X_train) // 50))\n",
        "\n",
        "    # Initialize CarbonTracker\n",
        "    tracker = EmissionsTracker()\n",
        "\n",
        "    # Start tracking cabron emissions\n",
        "    tracker.start()\n",
        "\n",
        "    # Start tracking time\n",
        "    start_time = time.time()\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=1000,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=2\n",
        "    )\n",
        "    #tracker.epoch_end()  # Stop tracking\n",
        "    emissions = tracker.stop()\n",
        "    emissions = emissions * 1000\n",
        "\n",
        "    # Stop tracking time\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time  # Time taken in seconds\n",
        "\n",
        "    # Number of epochs used before early stopping\n",
        "    num_epochs = len(history.history['loss'])\n",
        "\n",
        "    # Get final accuracy\n",
        "    final_accuracy = history.history['accuracy'][-1]\n",
        "\n",
        "    # Get F-score (using validation set predictions)\n",
        "    y_pred_prob = model.predict(X_val)\n",
        "    y_pred = np.argmax(y_pred_prob, axis=1)  # Convert probabilities to class labels\n",
        "    f_score = f1_score(y_val, y_pred, average='weighted')\n",
        "\n",
        "\n",
        "    # Generate training history graph\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Accuracy plot\n",
        "    ax[0].plot(history.history['accuracy'], label=\"Train Accuracy\")\n",
        "    ax[0].plot(history.history['val_accuracy'], label=\"Validation Accuracy\")\n",
        "    ax[0].set_title(\"Model Accuracy\")\n",
        "    ax[0].set_xlabel(\"Epoch\")\n",
        "    ax[0].set_ylabel(\"Accuracy\")\n",
        "    ax[0].legend()\n",
        "\n",
        "    # Loss plot\n",
        "    ax[1].plot(history.history['loss'], label=\"Train Loss\")\n",
        "    ax[1].plot(history.history['val_loss'], label=\"Validation Loss\")\n",
        "    ax[1].set_title(\"Model Loss\")\n",
        "    ax[1].set_xlabel(\"Epoch\")\n",
        "    ax[1].set_ylabel(\"Loss\")\n",
        "    ax[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the plot to a buffer\n",
        "    buffer = io.BytesIO()\n",
        "    plt.savefig(buffer, format=\"png\")\n",
        "    buffer.seek(0)\n",
        "\n",
        "    # Convert buffer to Anvil Media\n",
        "    graph_media = anvil.BlobMedia(\"image/png\", buffer.getvalue())\n",
        "\n",
        "    # Create a result string\n",
        "    result_output = (\n",
        "        f\"Estimated CO₂ emissions: {emissions:.6f} g\\n\"\n",
        "        f\"Number of Epochs: {num_epochs}\\n\"\n",
        "        f\"Time Taken: {total_time:.2f} seconds\\n\"\n",
        "        f\"Final Accuracy: {final_accuracy:.4f}\\n\"\n",
        "        f\"F-score: {f_score:.4f}\"\n",
        "    )\n",
        "\n",
        "    return result_output, graph_media"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWAPhCjV24QW"
      },
      "outputs": [],
      "source": [
        "def split_data(df, target_column):\n",
        "    # Separate features and target\n",
        "    X = df.drop(columns=[target_column]).values\n",
        "    y = df[target_column].values\n",
        "\n",
        "    # Split into training, validation, and test sets\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPisG3WRt1lX"
      },
      "outputs": [],
      "source": [
        "def encode_categorical_columns(df):\n",
        "  for column in df.columns:\n",
        "        if df[column].dtype == 'object' or pd.api.types.is_categorical_dtype(df[column]):\n",
        "            print(f\"Encoding column: {column}\")\n",
        "            label_encoder = LabelEncoder()\n",
        "            df[column] = label_encoder.fit_transform(df[column])\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRADIENT BOOSTING"
      ],
      "metadata": {
        "id": "pwp-Ss7yGWcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "# GRADIENT BOOSTING CLASSIFIER\n",
        "\n",
        "def build_train_and_track_emissions_gbc(X_train, y_train, X_val, y_val):\n",
        "    # Initialize the Gradient Boosting model with optimized parameters\n",
        "    model = GradientBoostingClassifier(\n",
        "        loss='log_loss',  # 'exponential' is another option\n",
        "        learning_rate=0.1,\n",
        "        n_estimators=100,\n",
        "        subsample=1.0,\n",
        "        criterion='friedman_mse',\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        max_depth=3,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Initialize CarbonTracker\n",
        "    tracker = EmissionsTracker()\n",
        "    tracker.start()\n",
        "\n",
        "    # Start tracking time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Stop tracking emissions\n",
        "    emissions = tracker.stop()\n",
        "    emissions = emissions * 1000  # Convert kg to grams\n",
        "\n",
        "    # Stop tracking time\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    # Get predictions\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Compute accuracy and F1-score\n",
        "    final_accuracy = accuracy_score(y_val, y_pred)\n",
        "    f_score = f1_score(y_val, y_pred, average=\"weighted\")\n",
        "\n",
        "    # Create a result string\n",
        "    result_output = (\n",
        "        f\"Estimated CO₂ emissions: {emissions:.6f} g\\n\"\n",
        "        f\"Time Taken: {total_time:.2f} seconds\\n\"\n",
        "        f\"Final Accuracy: {final_accuracy:.4f}\\n\"\n",
        "        f\"F-score: {f_score:.4f}\"\n",
        "    )\n",
        "\n",
        "    return result_output"
      ],
      "metadata": {
        "id": "phw7j1XSGXLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Random Forest*"
      ],
      "metadata": {
        "id": "1MJFgqMRnxO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "def build_train_and_track_emissions_rf(X_train, y_train, X_val, y_val):\n",
        "    # Initialize the Random Forest model\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)  # 100 trees in the forest\n",
        "\n",
        "    # Initialize CarbonTracker\n",
        "    tracker = EmissionsTracker()\n",
        "\n",
        "    # Start tracking carbon emissions\n",
        "    tracker.start()\n",
        "\n",
        "    # Start tracking time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Stop tracking emissions\n",
        "    emissions = tracker.stop() * 1000  # Convert to grams\n",
        "\n",
        "    # Stop tracking time\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    # Predict on the validation set\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "\n",
        "    # Calculate F-score\n",
        "    f_score = f1_score(y_val, y_pred, average='weighted')\n",
        "\n",
        "    # Create a result string\n",
        "    result_output = (\n",
        "        f\"Estimated CO₂ emissions: {emissions:.6f} g\\n\"\n",
        "        f\"Time Taken: {total_time:.2f} seconds\\n\"\n",
        "        f\"Final Accuracy: {accuracy:.4f}\\n\"\n",
        "        f\"F-score: {f_score:.4f}\"\n",
        "    )\n",
        "    return result_output"
      ],
      "metadata": {
        "id": "QKysH6LzdVOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFTnKmA92wbD"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def run_nn_for_classification_with_emissions(selected_column):\n",
        "\n",
        "    selected_column_index = df.columns.get_loc(selected_column)\n",
        "    target = df.columns[selected_column_index]\n",
        "\n",
        "\n",
        "    # Split data\n",
        "    encoded_df = encode_categorical_columns(df)\n",
        "\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(df, target)\n",
        "\n",
        "    # Train neural network and track emissions\n",
        "    input_size = X_train.shape[1]\n",
        "    output_size = df[target].nunique()\n",
        "\n",
        "    result_output, graph_media = build_train_and_track_emissions_nn(X_train, y_train, X_val, y_val, input_size, output_size)\n",
        "\n",
        "\n",
        "    return result_output, graph_media\n",
        "\n",
        "@anvil.server.callable\n",
        "def run_dt_for_classification_with_emissions(selected_column):\n",
        "\n",
        "    selected_column_index = df.columns.get_loc(selected_column)\n",
        "    target = df.columns[selected_column_index]\n",
        "\n",
        "\n",
        "    # Split data\n",
        "    encoded_df = encode_categorical_columns(df)\n",
        "\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(df, target)\n",
        "\n",
        "    # Train neural network and track emissions\n",
        "    input_size = X_train.shape[1]\n",
        "    output_size = df[target].nunique()\n",
        "\n",
        "    tracker = build_train_and_track_emissions_dt(X_train, y_train, X_val, y_val, input_size, output_size)\n",
        "\n",
        "\n",
        "    return tracker\n",
        "\n",
        "\n",
        "@anvil.server.callable\n",
        "def run_svm_for_classification_with_emissions(selected_column):\n",
        "\n",
        "    selected_column_index = df.columns.get_loc(selected_column)\n",
        "    target = df.columns[selected_column_index]\n",
        "\n",
        "\n",
        "    # Split data\n",
        "    encoded_df = encode_categorical_columns(df)\n",
        "\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(df, target)\n",
        "\n",
        "    # Train neural network and track emissions\n",
        "    input_size = X_train.shape[1]\n",
        "    output_size = df[target].nunique()\n",
        "\n",
        "    tracker = build_train_and_track_emissions_svm(X_train, y_train, X_val, y_val, input_size, output_size)\n",
        "\n",
        "\n",
        "    return tracker\n",
        "\n",
        "@anvil.server.callable\n",
        "def run_gbc_for_classification_with_emissions(selected_column):\n",
        "    selected_column_index = df.columns.get_loc(selected_column)\n",
        "    target = df.columns[selected_column_index]\n",
        "\n",
        "    # Encode categorical data\n",
        "    encoded_df = encode_categorical_columns(df)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(df, target)\n",
        "\n",
        "    # Get input/output sizes\n",
        "    input_size = X_train.shape[1]\n",
        "    output_size = df[target].nunique()\n",
        "\n",
        "    # Train Gradient Boosting Classifier and track emissions\n",
        "    tracker = build_train_and_track_emissions_gbc(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    return tracker\n",
        "\n",
        "\n",
        "@anvil.server.callable\n",
        "def run_rf_for_classification_with_emissions(selected_column):\n",
        "\n",
        "    selected_column_index = df.columns.get_loc(selected_column)\n",
        "    target = df.columns[selected_column_index]\n",
        "\n",
        "\n",
        "    # Split data\n",
        "    encoded_df = encode_categorical_columns(df)\n",
        "\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(df, target)\n",
        "\n",
        "    # Train neural network and track emissions\n",
        "    input_size = X_train.shape[1]\n",
        "    output_size = df[target].nunique()\n",
        "\n",
        "    tracker = build_train_and_track_emissions_rf(X_train, y_train, X_val, y_val)\n",
        "\n",
        "\n",
        "    return tracker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebpfh7cqIE-f"
      },
      "source": [
        "#Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irDCvFGyF1Zv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "464a221e-2671-4712-ca9b-9880faaba1d9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'carbontracker'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-8192d03284fb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcarbontracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCarbonTracker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# Modified one (MSE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'carbontracker'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\n",
        "import io\n",
        "import sys\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from carbontracker.tracker import CarbonTracker\n",
        "# Modified one (MSE)\n",
        "\n",
        "\n",
        "def build_train_and_track_emissions1(X_train, y_train, X_val, y_val, input_size):\n",
        "    hidden_layer_size = 50  # Number of neurons in the hidden layer\n",
        "\n",
        "    # Define the model\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(hidden_layer_size, activation='relu', input_shape=(input_size,)),\n",
        "        tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "        tf.keras.layers.Dense(1)  # Single output for regression\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])  # Use MSE for regression\n",
        "\n",
        "    # Early stopping to prevent overfitting\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
        "\n",
        "    # Redirect CarbonTracker output\n",
        "    buffer = io.StringIO()\n",
        "    sys.stdout = buffer\n",
        "\n",
        "    # Initialize CarbonTracker\n",
        "    tracker = CarbonTracker(epochs=100)\n",
        "\n",
        "    # Train the model with CarbonTracker\n",
        "    tracker.epoch_start()  # Start tracking\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=2\n",
        "    )\n",
        "    tracker.epoch_end()  # Stop tracking\n",
        "\n",
        "    # Reset stdout\n",
        "    sys.stdout = sys.__stdout__\n",
        "\n",
        "    # Extract relevant lines from the tracker output\n",
        "    output = buffer.getvalue()\n",
        "    buffer.close()\n",
        "\n",
        "    # Parse the output for the desired section\n",
        "    relevant_lines = []\n",
        "    capturing = False\n",
        "    for line in output.split(\"\\n\"):\n",
        "        if \"Actual consumption\" in line or \"Predicted consumption\" in line:\n",
        "            capturing = True\n",
        "        if capturing:\n",
        "            relevant_lines.append(line)\n",
        "            if \"travelled by car\" in line:  # End after the car equivalence line\n",
        "                break\n",
        "\n",
        "    # Get the final MSE from the training history\n",
        "    final_mse = history.history['mse'][-1]  # Get the last MSE value from the training history\n",
        "\n",
        "    # Create a normal output string\n",
        "    result_output = \"\\n\".join(relevant_lines) + f\"\\nFinal Mean Squared Error (MSE): {final_mse}\"\n",
        "\n",
        "    return result_output\n",
        "\n",
        "def split_data1(df, target_column):\n",
        "    # Separate features and target\n",
        "    X = df.drop(columns=[target_column]).values\n",
        "    y = df[target_column].values\n",
        "\n",
        "    # Split into training, validation, and test sets\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "def encode_categorical_columns1(df):\n",
        "    for column in df.columns:\n",
        "        if df[column].dtype == 'object' or pd.api.types.is_categorical_dtype(df[column]):\n",
        "            print(f\"Encoding column: {column}\")\n",
        "            label_encoder = LabelEncoder()\n",
        "            df[column] = label_encoder.fit_transform(df[column])\n",
        "    return df\n",
        "\n",
        "@anvil.server.callable\n",
        "def run_nn_for_regression_with_emissions1(selected_column):\n",
        "    global df\n",
        "    selected_column_index = df.columns.get_loc(selected_column)\n",
        "    target = df.columns[selected_column_index]\n",
        "\n",
        "\n",
        "    # Split data\n",
        "    encoded_df = encode_categorical_columns1(df)\n",
        "\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = split_data1(encoded_df, target)\n",
        "\n",
        "    # Train neural network and track emissions\n",
        "    input_size = X_train.shape[1]\n",
        "    tracker = build_train_and_track_emissions1(X_train, y_train, X_val, y_val, input_size)\n",
        "\n",
        "    # Return results\n",
        "    return tracker\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}